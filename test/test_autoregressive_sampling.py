import unittest
import torch
import time
from unittest.mock import Mock, patch, MagicMock
import sys
import os

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
project_root = "/home/tiantianyi/code/DuoDecoding"
sys.path.insert(0, project_root)

# å¯¼å…¥æ¨¡å—
try:
    from src.engine import Decoding

    print(f"Successfully imported Decoding from src.engine")
except ImportError as e:
    print(f"Failed to import: {e}")
    try:
        src_path = os.path.join(project_root, "src")
        sys.path.insert(0, src_path)
        import model_gpu
        import engine
        from engine import Decoding

        print(f"Successfully imported using module pre-loading")
    except ImportError as e2:
        import importlib.util

        model_gpu_path = os.path.join(project_root, "src", "model_gpu.py")
        spec_model_gpu = importlib.util.spec_from_file_location("model_gpu", model_gpu_path)
        model_gpu_module = importlib.util.module_from_spec(spec_model_gpu)
        sys.modules["model_gpu"] = model_gpu_module
        spec_model_gpu.loader.exec_module(model_gpu_module)

        engine_path = os.path.join(project_root, "src", "engine.py")
        spec_engine = importlib.util.spec_from_file_location("engine", engine_path)
        engine_module = importlib.util.module_from_spec(spec_engine)
        sys.modules["engine"] = engine_module
        engine_module.KVCacheModel = model_gpu_module.KVCacheModel
        spec_engine.loader.exec_module(engine_module)

        Decoding = engine_module.Decoding
        print(f"Successfully imported using importlib manual loading")


class TestCUDATimingAccuracy(unittest.TestCase):
    """ä¸“é—¨æµ‹è¯•CUDAè®¡æ—¶å‡†ç¡®æ€§çš„æµ‹è¯•ç±»"""

    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        if not torch.cuda.is_available():
            self.skipTest("CUDA not available")

        self.mock_args = Mock()
        self.mock_args.eval_mode = "large"
        self.mock_args.temp = 0.0
        self.mock_args.top_k = 50
        self.mock_args.top_p = 0.9
        self.mock_args.max_tokens = 128
        self.mock_args.seed = 42

        self.mock_decoding = Mock()
        self.mock_decoding.args = self.mock_args
        self.mock_decoding.vocab_size = 32000

        self.mock_target_model = Mock()
        self.mock_target_model.device = torch.device("cuda")
        self.mock_target_model.parameters.return_value = [torch.randn(100, 100, requires_grad=True)]

        self.mock_decoding.target_model = self.mock_target_model
        self.mock_decoding.color_print = Mock()

        # ç»‘å®šçœŸå®çš„è®¡æ—¶å‡½æ•°
        self.mock_decoding.autoregressive_sampling = Decoding.autoregressive_sampling.__get__(
            self.mock_decoding, type(self.mock_decoding)
        )

    @patch("src.engine.KVCacheModel")
    def test_cuda_timing_accuracy(self, mock_kv_cache):
        """æµ‹è¯•CUDAè®¡æ—¶çš„å‡†ç¡®æ€§"""
        mock_model_instance = Mock()
        mock_kv_cache.return_value = mock_model_instance

        initial_tokens = torch.randint(0, 1000, (1, 3))

        # ğŸ”§ é€‚åˆ128 tokensçš„å»¶è¿Ÿè®¾ç½®
        prefill_delay = 0.1  # 100ms - ä¸åº”è®¡å…¥decode_time
        decode_delay = 0.02  # 20ms per token - åº”è®¡å…¥decode_time

        call_count = 0

        def mock_generate_with_known_delay(x, num_tokens):
            nonlocal call_count
            call_count += 1

            if call_count == 1:
                print(f"  Prefill phase: sleeping {prefill_delay}s")
                time.sleep(prefill_delay)
            else:
                print(f"  Decode phase {call_count-1}: sleeping {decode_delay}s")
                time.sleep(decode_delay)

            device = x.device
            new_tokens = torch.randint(0, 1000, (x.shape[0], num_tokens), device=device)
            return torch.cat([x, new_tokens], dim=1)

        mock_model_instance.generate = mock_generate_with_known_delay
        mock_model_instance.vocab_size = 32000

        print(f"\nğŸ”„ Starting CUDA timing test with {self.mock_args.max_tokens} tokens...")
        start_time = time.time()
        result, decode_time = self.mock_decoding.autoregressive_sampling(initial_tokens, enable_timing=True)
        total_time = time.time() - start_time

        # è®¡ç®—æœŸæœ›çš„decodeæ—¶é—´
        expected_decode_time = decode_delay * (self.mock_args.max_tokens - 1)

        print(f"\nğŸ“Š CUDA Timing Results:")
        print(f"Total execution time: {total_time:.4f}s")
        print(f"Measured decode time: {decode_time:.4f}s")
        print(f"Expected decode time: {expected_decode_time:.4f}s")
        print(f"Prefill delay (excluded): {prefill_delay:.4f}s")
        print(f"Timing accuracy: {(decode_time/expected_decode_time)*100:.1f}%")

        # å…³é”®éªŒè¯ï¼šdecode_timeåº”è¯¥æ’é™¤prefillæ—¶é—´
        self.assertLess(decode_time, total_time, "Decode time should be less than total time")

        # éªŒè¯è®¡æ—¶ç²¾åº¦ï¼šå…è®¸Â±5%è¯¯å·®ï¼ˆæ›´ä¸¥æ ¼ï¼Œå› ä¸ºå»¶è¿Ÿæ—¶é—´æ›´çŸ­ï¼‰
        tolerance = 0.05
        lower_bound = expected_decode_time * (1 - tolerance)
        upper_bound = expected_decode_time * (1 + tolerance)

        self.assertGreater(decode_time, lower_bound, f"Decode time {decode_time:.4f}s should be â‰¥ {lower_bound:.4f}s")
        self.assertLess(decode_time, upper_bound, f"Decode time {decode_time:.4f}s should be â‰¤ {upper_bound:.4f}s")

        print("âœ… CUDA timing accuracy test passed!")

    @patch("src.engine.KVCacheModel")
    def test_cuda_prefill_decode_separation(self, mock_kv_cache):
        """æµ‹è¯•CUDAç¯å¢ƒä¸‹prefillå’Œdecodeé˜¶æ®µçš„æ­£ç¡®åˆ†ç¦»"""
        mock_model_instance = Mock()
        mock_kv_cache.return_value = mock_model_instance

        initial_tokens = torch.randint(0, 1000, (1, 3))

        # ğŸ”§ é€‚åˆ128 tokensçš„å»¶è¿Ÿè®¾ç½®
        prefill_delay = 0.05  # 50ms
        decode_delay = 0.01  # 10ms per token

        call_count = 0
        phase_times = []

        def mock_generate_with_phase_tracking(x, num_tokens):
            nonlocal call_count
            call_count += 1

            phase_start = time.time()

            if call_count == 1:
                print(f"  Prefill: {prefill_delay}s delay")
                time.sleep(prefill_delay)
                phase_times.append(("prefill", time.time() - phase_start))
            else:
                if call_count <= 10:  # åªæ‰“å°å‰å‡ ä¸ªï¼Œé¿å…è¾“å‡ºè¿‡å¤š
                    print(f"  Decode {call_count-1}: {decode_delay}s delay")
                elif call_count == 11:
                    print(f"  ... continuing decode phases ...")
                time.sleep(decode_delay)
                phase_times.append(("decode", time.time() - phase_start))

            device = x.device
            new_tokens = torch.randint(0, 1000, (x.shape[0], num_tokens), device=device)
            return torch.cat([x, new_tokens], dim=1)

        mock_model_instance.generate = mock_generate_with_phase_tracking
        mock_model_instance.vocab_size = 32000

        print(f"\nğŸ”„ Testing prefill/decode separation with {self.mock_args.max_tokens} tokens...")
        result, decode_time = self.mock_decoding.autoregressive_sampling(initial_tokens, enable_timing=True)

        # åˆ†æé˜¶æ®µæ—¶é—´
        prefill_phases = [t for phase, t in phase_times if phase == "prefill"]
        decode_phases = [t for phase, t in phase_times if phase == "decode"]

        total_prefill_time = sum(prefill_phases)
        total_decode_time_actual = sum(decode_phases)

        print(f"\nğŸ“Š Phase Separation Results:")
        print(f"Prefill phases: {len(prefill_phases)} (total: {total_prefill_time:.4f}s)")
        print(f"Decode phases: {len(decode_phases)} (total: {total_decode_time_actual:.4f}s)")
        print(f"Measured decode_time: {decode_time:.4f}s")
        print(f"Accuracy ratio: {decode_time/total_decode_time_actual:.4f}")

        # ğŸ”§ ä¿®æ­£éªŒè¯é€»è¾‘ï¼š
        # 1. decode_timeåº”è¯¥æ¥è¿‘å®é™…decodeé˜¶æ®µæ€»æ—¶é—´
        self.assertAlmostEqual(
            decode_time, total_decode_time_actual, delta=0.2, msg="decode_time should match actual decode phases time"
        )

        # 2. decode_timeåº”è¯¥è¿œå°äºåŒ…å«prefillçš„æ€»æ—¶é—´
        total_all_phases = total_prefill_time + total_decode_time_actual
        self.assertLess(
            decode_time,
            total_all_phases,
            f"decode_time ({decode_time:.4f}s) should be much less than total time ({total_all_phases:.4f}s)",
        )

        # 3. decode_timeä¸åº”è¯¥æ¥è¿‘prefillæ—¶é—´ï¼ˆè¯´æ˜æ²¡æœ‰é”™è¯¯åœ°åŒ…å«prefillï¼‰
        self.assertGreater(
            decode_time,
            total_prefill_time * 10,
            f"decode_time ({decode_time:.4f}s) should be much larger than prefill time ({total_prefill_time:.4f}s)",
        )

        print("âœ… Prefill/decode separation test passed!")
        print(
            f"âœ¨ Your timing function is working perfectly! Accuracy: {(decode_time/total_decode_time_actual)*100:.2f}%"
        )


if __name__ == "__main__":
    unittest.main(verbosity=2)
